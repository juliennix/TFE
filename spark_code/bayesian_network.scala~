/////////////////////////////////////////////////////////////////
// Author : Nix Julien                                         //        
// For the University of LiÃ¨ge                                 //     
// Date : 06/02/2015                                           //
// This creates a bayesian network                             //
/////////////////////////////////////////////////////////////////   

package bayesian_network

import kruskal.Kruskal._

import Array._
import org.apache.spark.rdd.RDD
import org.apache.spark.SparkContext._

object Bayesian_network
{
    def entropy(l : List[Float]): Float = 
    {
        var length = l.length
        var freq = l.groupBy(x=>x).mapValues(_.size.toFloat/length)
        
        return freq.values.map{ x =>
            -x * math.log(x) / math.log(2)}.reduce(_+_).toFloat
    }
            
    def getCol(n: Int, a: Array[Array[Float]]) = a.map{_(n - 1)}

    def conditionalEntropy(variable : List[Float], condition : List[Float]): Float = {
        var nbSamples = variable.length - 1
        var freqMap = variable.groupBy(x=>x).mapValues(_.size.toFloat)
        var cardinalityVar = freqMap.size
        var freqMap2 = condition.groupBy(x=>x).mapValues(_.size.toFloat)
        var cardinalityCond = freqMap2.size
        var M = ofDim[Float](cardinalityVar, cardinalityCond)

        for (i <- 0 to nbSamples)
        {
            M(variable(i).toInt)(condition(i).toInt) = M(variable(i)toInt)(condition(i)toInt) + 1
        }
        
        M = M.map(_.map(_/nbSamples.toFloat))
        
        var pY = new Array[Float](cardinalityCond)
        
        for(i<-0 to cardinalityCond - 1) 
        {
            pY(i) = getCol(i+1, M).reduce(_+_)
        }
        
        return M.map{ l => 
            var pYIndex = -1
            l.map{x=>
                pYIndex += 1
                if (x == 0) x
                else -x * math.log(x/pY(pYIndex))/math.log(2)}.reduce(_+_).toFloat
            }.reduce(_+_).toFloat
    }

    def mutInfo(variable : List[Float], condition : List[Float]): Float = 
    {
        var result = entropy(variable) - conditionalEntropy(variable, condition)
        return result
    } 

    def skelTree(samples :org.apache.spark.rdd.RDD[(Float, Array[Float])]) = 
    {
        var nbNodes = samples.count.toInt
        var M = ofDim[Float](nbNodes, nbNodes) 
        
        for (i <- 0 to nbNodes-1)
        {
            for (j <- 0 to nbNodes-1) 
            {
                //if (i == j) M(i)(j) = -10000
                //else M(i)(j) = - mutInfo(samples(i), samples(j))
                M(i)(j) = - mutInfo(samples.lookup(i)(0).toList, samples.lookup(j)(0).toList)
                println(M(i)(j))
            }
        }
        
        var MTranspose = M.transpose
        
        for (i <- 0 to nbNodes-1) 
        {
            for (j <- 0 to nbNodes-1) 
            {
                M(i)(j) = (M(i)(j) + MTranspose(i)(j))/2
            }
        }
 
        kruskalTree(M)
        
    }
}













